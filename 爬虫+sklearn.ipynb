{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=['http://news.baidu.com/n?cmd=4&class=mil&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=finannews&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=internet&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=housenews&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=autonews&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=sportnews&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=enternews&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=gamenews&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=edunews&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=healthnews&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=technnews&pn=1&from=tab'\\\n",
    "     ,'http://news.baidu.com/n?cmd=4&class=socianews&pn=1&from=tab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=['E:/baidu/军事.txt'\\\n",
    "    ,'E:/baidu/财经.txt'\\\n",
    "    ,'E:/baidu/互联网.txt'\\\n",
    "    ,'E:/baidu/房产.txt'\\\n",
    "    ,'E:/baidu/汽车.txt'\\\n",
    "    ,'E:/baidu/体育.txt'\\\n",
    "    ,'E:/baidu/娱乐.txt'\\\n",
    "    ,'E:/baidu/游戏.txt'\\\n",
    "    ,'E:/baidu/教育.txt'\\\n",
    "    ,'E:/baidu/女人.txt'\\\n",
    "    ,'E:/baidu/科技.txt'\\\n",
    "    ,'E:/baidu/社会.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,8):\n",
    "    soup = BeautifulSoup(urllib2.urlopen(url[j]).read())\n",
    "    main = soup.find('div',{'class':'p2'})\n",
    "    index = main.find_all('a')\n",
    "    len_0 = len(index)\n",
    "    a = []\n",
    "    for i in range(len_0):\n",
    "        a.append(index[i]['href'])\n",
    "\n",
    "    for i in range(len_0):\n",
    "        try:\n",
    "            soup = BeautifulSoup(urllib2.urlopen(a[i]).read())\n",
    "            txt = soup.find(text=re.compile(ur\"[\\u4e00-\\u9fa5]+\"))\n",
    "            txt_ = ''.join(txt)\n",
    "            f = open(ff[j],'a')\n",
    "            print >>f,txt_\n",
    "            f.close()\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "def load_data():\n",
    "    corpus_train = []\n",
    "    target_train = []\n",
    "    filepath = ''\n",
    "    filelist = os.listdir(filepath)\n",
    "    for num in range(len(filelist)):\n",
    "        filetext = filepath+\"/\"+filelist[num]\n",
    "        filename = os.path.basename(filetext)\n",
    "        myfile = open(filetext,'r','utf-8')\n",
    "        temp = myfile.readlines()\n",
    "        myfile.close()\n",
    "        seg_list = jieba.cut(','.join(temp[1:]),cut_all=False)\n",
    "        words = \" \".join(seg_list)\n",
    "        target_train.append(filename)\n",
    "        corpus_train.append(words)\n",
    "     #-------#\n",
    "    corpus_test = []\n",
    "    target_test = []\n",
    "    filepath = ''\n",
    "    filelist = os.listdir(filepath)\n",
    "    for num in range(len(filelist)):\n",
    "        filetext = filepath+\"/\"+filelist[num]\n",
    "        myfile = open(filetext,'r')\n",
    "        temp = myfile.readlines()\n",
    "        myfile.close()\n",
    "        seg_list = jieba.cut(','.join(temp[1:]),cut_all=False)\n",
    "        words = \" \".join(seg_list)\n",
    "        target_train.append(temp[0])\n",
    "        corpus_test.append(words)\n",
    "    return [[corpus_train,target_train],[corpus_test,target_test]]\n",
    "\n",
    "def data_pro():\n",
    "    [[corpus_train, target_train], [corpus_test, target_test]] = load_data()\n",
    "    count_v1 = CountVectorizer()\n",
    "    #该类会讲文本的词语转化为词频矩阵，矩阵元素啊a[i][j]表示j词在i类文本下的词频\n",
    "    counts_train = count_v1.fit_transform(corpus_train)\n",
    "\n",
    "    transformer = TfidfVectorizer()\n",
    "    tfidf_train = transformer.fit(counts_train).transform(counts_train)\n",
    "\n",
    "    weight_train = tfidf_train.toarray()\n",
    "    count_v2 = CountVectorizer(vocabulary=count_v1.vocabulary_)\n",
    "    # 让两个CountVectorizer共享vocabulary\n",
    "\n",
    "    counts_test = count_v2.fit_transform(corpus_test)\n",
    "    # fit_transform是将文本转为词频矩阵\n",
    "\n",
    "    transformer = TfidfVectorizer()\n",
    "    # 该类会统计每个词语的tf-idf权值\n",
    "\n",
    "    tfidf_test = transformer.fit(counts_train).transform(counts_test)\n",
    "    # fit_transform是计算tf-idf\n",
    "\n",
    "    weight_test = tfidf_test.toarray()\n",
    "    # weight[i][j],第i个文本，第j个词的tf-idf值\n",
    "    return [[weight_train, target_train], [weight_test, target_test]]\n",
    "        \n",
    "[[weight_train, target_train], [weight_test, target_test]] = data_pro()\n",
    "\n",
    "# ---------------------------------------------#\n",
    "knnclf = KNeighborsClassifier()\n",
    "knnclf.fit(weight_train, target_train)\n",
    "knn_pred = knnclf.predict(weight_test)\n",
    "# knn模型\n",
    "# ---------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------#\n",
    "# svm模型\n",
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(weight_train, target_train)\n",
    "svc_pred = svc.predict(weight_test)\n",
    "# ---------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------#\n",
    "# tree模型\n",
    "tre = tree.DecisionTreeClassifier()\n",
    "tre.fit(weight_train, target_train)\n",
    "tre_pred = tre.predict(weight_test)\n",
    "# ---------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------#\n",
    "# bayes模型\n",
    "bayes = MultinomialNB(alpha=0.01)\n",
    "bayes.fit(weight_train, target_train)\n",
    "bayes_pred = bayes.predict(weight_test)\n",
    "# ---------------------------------------------#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}